{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d549cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hossein\\University\\Data-Science\\Deep-Learning\\kunskapskontroll_2_DL\\DL2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import ollama\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298a5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Hello, Llama!\"}])\n",
    "\n",
    "local_model = \"llama3.2\"\n",
    "api_model = \"llama-3.3-70b-versatile\"\n",
    "api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "api_key = \"\"\n",
    "\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=200, chunk_overlap=40):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    print(\"Chunking completed.\")\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks)\n",
    "    print(\"Embedding completed.\")\n",
    "    return embeddings\n",
    "\n",
    "def semantic_search(query, chunks, chunk_embeddings, model_name=\"all-MiniLM-L6-v2\", top_k=5):\n",
    "    print(\"Semantic searching ...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_embedding = model.encode([query])\n",
    "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    top_chunks = [chunks[i] for i in top_indices]\n",
    "    print(\"Semantic search completed.\")\n",
    "    return \"\\n\".join(top_chunks)\n",
    "\n",
    "def pull_local_model(model=local_model):\n",
    "    try:\n",
    "        ollama.pull(model)\n",
    "        print(f\"{model} model pulled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling {model}:\", e)\n",
    "\n",
    "def call_local_model(prompt, model=local_model):\n",
    "    pull_local_model(model)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = ollama.chat(model=model, messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "def call_api_model(prompt, api_url, api_key=None, api_model=api_model):\n",
    "    \"\"\"\n",
    "    Handles calling the remote API model.\n",
    "    \"\"\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "\n",
    "    response = requests.post(\n",
    "        api_url,\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"model\": api_model,\n",
    "            \"prompt\": prompt\n",
    "        },\n",
    "        timeout=10\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    api_result = response.json()\n",
    "\n",
    "    # Support OpenAI-style or custom response formats\n",
    "    if \"response\" in api_result:\n",
    "        return api_result[\"response\"]\n",
    "    elif \"choices\" in api_result and isinstance(api_result[\"choices\"], list):\n",
    "        return api_result[\"choices\"][0].get(\"message\", {}).get(\"content\", \"No content\")\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized API response format.\")\n",
    "\n",
    "def evaluate_relevance(response, reference_answer, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb1 = model.encode([response])[0]\n",
    "    emb2 = model.encode([reference_answer])[0]\n",
    "    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    return similarity  # closer to 1.0 is more relevant\n",
    "\n",
    "def detect_hallucination(response, context):\n",
    "    return all(sentence.lower() in context.lower() for sentence in response.split('.') if sentence.strip())\n",
    "\n",
    "def measure_latency(func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    return result, end - start\n",
    "\n",
    "def collect_user_feedback():\n",
    "    score = input(\"Rate response from 1 (bad) to 5 (great): \")\n",
    "    return int(score)\n",
    "\n",
    "def answer_length(response):\n",
    "    return len(response.split())\n",
    "\n",
    "def coverage_score(response, context):\n",
    "    response_words = set(response.lower().split())\n",
    "    context_words = set(context.lower().split())\n",
    "    return len(response_words & context_words) / len(response_words)\n",
    "\n",
    "def get_response(prompt, api_url=api_url, api_key=None, api_model=api_model, local_model=local_model):\n",
    "    \"\"\"\n",
    "    Get a response from LLaMA. Prefer API, fallback to local Ollama model if API fails.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    # Try API first\n",
    "    if api_url:\n",
    "        try:\n",
    "            return call_api_model(prompt, api_url, api_key, api_model)\n",
    "        except Exception as e:\n",
    "            print(f\"[API Error] {e} â€“ falling back to local model.\")\n",
    "\n",
    "    # Fallback: local Ollama model\n",
    "    try:\n",
    "        return call_local_model(prompt, local_model)\n",
    "    except Exception as e:\n",
    "        print(f\"[Local Model Error] {e}\")\n",
    "        return \"Error: Could not get a response from either the API or the local model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_pdf(\"HIAI_Company_Profile.pdf\")\n",
    "chunks = chunk_text(text, 100, 20)\n",
    "embeddings = generate_embeddings(chunks)\n",
    "query = \"how many products does this company have name them?\"\n",
    "context = semantic_search(query, chunks, embeddings)\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cac36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = call_local_model(prompt)\n",
    "response = get_response(prompt, api_url, api_key, api_model, local_model)\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
